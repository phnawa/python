{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ##忽略警告信息\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "path_prefix = 'E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(path='E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\training_nolabel.txt'):\n",
    "    # 把training时需要的data读进来\n",
    "    # 如果是'training_label.txt'，需要读取label，如果是'training_nolabel.txt'，不需要读取label\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()  ##依次读取每行\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]  ##移除每行头尾的换行符并以空格为分隔符\n",
    "        x = [line[2:] for line in lines]    ##对于每行的每个词，都从第三个字符开始提取\n",
    "        y = [line[0] for line in lines]     ##对于每行的每个词，只提取第一个字符\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\testing_data.txt'):\n",
    "    # 把testing时需要的data读进来\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]#将test_data分成许多行，从第二行开始，去除每行首尾的换行符并以“，”号\n",
    "                                           #将每行分成许多个小部分，并用“”将各个小部分连接起来\n",
    "        X = [sen.split(' ') for sen in X]  ##将上述处理好的文本的每行以空格符为标志将每行分割成若干小部分。\n",
    "    return X ##返回的是一个二维的list\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1 # 大于等于0.5为有恶意\n",
    "    outputs[outputs<0.5] = 0 # 小于0.5为无恶意\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()##torch.eq()函数比较两个tensor是否相等，相等的地方写1，不相等的地方写0\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7 face=\"黑体\">Train Word To Vector:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "def train_word2vec(x):\n",
    "    # 训练 word to vector 的 word embedding\n",
    "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)##x为输入语料集，size为特征向量的维度， \n",
    "    #window：表示当前词与预测词在一个句子中的最大距离是多少 ，min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5 \n",
    "    #workers参数控制训练的并行数， iter： 迭代次数， sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\training_label.txt')\n",
    "    train_x_no_label = load_training_data('E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\training_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('E:\\\\BaiduNetdiskDownload\\\\数据\\\\hw4\\\\testing_data.txt')\n",
    "\n",
    "    model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
    "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7 face=\"黑体\">Data Preprocess:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
